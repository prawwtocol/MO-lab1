"""
Часть идей почерпнута из


https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D1%81%D1%82%D0%BE%D0%B2%D1%8B%D0%B5_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8_%D0%B4%D0%BB%D1%8F_%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8
"""

from typing import Any

import numpy as np

from custom_types import FuncType
from util import _get_lib

"""
Эллиптический параболоид 

Здесь я менял число обусловленности cond = 10, 100, 250

Вывод: Увеличение числа обусловленности приводит к численной нестабильности в простых методах

Наблюдения:
При cond=250 и LR=0.01 Const LR, ExpDecay LR и StepDecay LR расходятся, выдавая NaN/Inf
При этом корректно выставляется status = -1

При этом LineSearch GS остается стабильным и находит решение всего за 9 итераций
Это показывает, что адаптация шага на каждой итерации — мощный инструмент для борьбы
с плохой обусловленностью

GD (Armijo LS) также сходится, но при cond=250 ему требуется уже предельно много итераций (approx 2000), что говорит о его меньшей эффективности в таких  условиях 
"""


def elliptic_paraboloid(
    x: float, y: float, c1: float = 1, c2: float = 1, **kwargs: Any
) -> float:
    """
    Эллиптический параболоид - квадратичная функция
    f(x,y) = c1*x^2 + c2*y^2
    Минимум в (0,0), f(0,0) = 0

    В эллиптическом параболоиде f(x,y) = c1*x^2 + c2*y^2 Гессиан представляет собой
    диагональную матрицу [[2*c1, 0], [0, 2*c2]]
    Собственные числа такой матрицы - это просто элементы на диагонали,
    то есть 2*c1 и 2*c2
    Число обусловленности равно max(c1,c2)/min(c1,c2)
    Эти коэффициенты определяют "вытянутость" параболоида вдоль осей
    координат, что влияет на сложность оптимизации.

    Большое число обусловленности означает, что функция сильно вытянута в
    одном направлении. Это делает оптимизацию сложнее,
    так как градиентный спуск с постоянным шагом может отскакивать от
    стенок долины, замедляя сходимость или даже вызывая расходимость.
    Чем выше число обусловленности, тем сложнее задача оптимизации и
    тем более продвинутые методы
    требуются для эффективного решения.


    Я исследую эллиптический параболоид с числами обусловленности 10, 100, 250
    Вывод: Увеличение числа обусловленности приводит к численной нестабильности в простых методах.
    """
    return c1 * x**2 + c2 * y**2


def elliptic_paraboloid_gradient(
    x: float, y: float, c1: float = 1, c2: float = 1, **kwargs: Any
):
    """
    Градиент эллиптического параболоида.
    """
    lib = _get_lib(x)
    df_dx = 2 * c1 * x
    df_dy = 2 * c2 * y
    return lib.array([df_dx, df_dy]) if lib == np else lib.stack([df_dx, df_dy])


"""
Функция Экли


Вывод: результат сильно зависит от начальной точки. 

Все исследованные методы являются локальными - То есть принимает решение, исходя из информации о значении градиенте функции только в текущей точке. Больше никакую информацию не использует для принятия решения. 
Однако методы с линейным поиском иногда показывают способность "выпрыгивать" из локальных минимумов.



Наблюдения:

Большинство методов, стартуя из точки [2.5, 2.5], застревают в ближайшем локальном минимуме.
Интересно: LineSearch GS и Armijo LS из той же точки [2.5, 2.5] смогли найти глобальный минимум в (0,0). 

Вероятно, адаптивный выбор большого шага на первых итерациях позволил им "перепрыгнуть" барьер локального минимума. 

Это является преимуществом данных методов.

При старте близко к глобальному минимуму ([0.1, -0.2]) многие методы находят его, но простой GD с постоянным шагом и Torch упираются в лимит итераций. Градиент в окрестности минимума очень мал, и они не могут сделать финальные шаги.
"""


def ackley_function(
    x1: float, x2: float, a: float = 20, b: float = 0.2, c: float = 2 * np.pi
) -> float:
    """
    Функция Экли.
    a, b, c -- параметры функции.
    Глобальный минимум в f(0,0) = 0.

    Это классическая тестовая функция в оптимизации. Её особенность в том, что она имеет широкую, почти плоскую внешнюю область и множество маленьких локальных минимумов, которые "усеивают" поверхность. В центре находится глобальный минимум
    """
    lib = _get_lib(x1)
    term1 = -a * lib.exp(-b * lib.sqrt(0.5 * (x1**2 + x2**2)))
    term2 = -lib.exp(0.5 * (lib.cos(c * x1) + lib.cos(c * x2)))
    return term1 + term2 + a + lib.exp(lib.tensor(1.0) if lib != np else 1.0)


def ackley_function_gradient(
    x1: float, x2: float, a: float = 20, b: float = 0.2, c: float = 2 * np.pi
):
    """
    Градиент функции Экли.
    """
    lib = _get_lib(x1)

    sqrt_term = lib.sqrt(0.5 * (x1**2 + x2**2))

    if sqrt_term == 0:
        # предотвращает деление на ноль
        df_dx1_term1 = 0
        df_dx2_term1 = 0
    else:
        df_dx1_term1 = a * b * (0.5 * x1 / sqrt_term) * lib.exp(-b * sqrt_term)
        df_dx2_term1 = a * b * (0.5 * x2 / sqrt_term) * lib.exp(-b * sqrt_term)

    common_exp_term2 = lib.exp(0.5 * (lib.cos(c * x1) + lib.cos(c * x2)))
    df_dx1_term2 = 0.5 * c * lib.sin(c * x1) * common_exp_term2
    df_dx2_term2 = 0.5 * c * lib.sin(c * x2) * common_exp_term2

    df_dx1 = df_dx1_term1 + df_dx1_term2
    df_dx2 = df_dx2_term1 + df_dx2_term2

    return lib.array([df_dx1, df_dx2]) if lib == np else lib.stack([df_dx1, df_dx2])


"""
Зашумленная Функция Бута

Здесь к значениям функции добавлялся шум, что усложняет задачу для методов, которые опираются на точное значение функции.

Вывод: Шум в целевой функции сильнее всего бьет по методам, которые активно ее вычисляют (линейный поиск, методы нулевого порядка)

Наблюдения:

Методы с линейным поиском (LineSearch GS) по-прежнему находят минимум, но количество вычислений функции (func_evals) на одной из траекторий взлетает с 229 до 76001! 
Алгоритм ищет на зашумленной поверхности  оптимальноый шаг, что оказывается очень затратно.

SciPy (Nelder-Mead), метод нулевого порядка, перестает справляться и упирается в лимит итераций. Это ожидаемо, так как он полностью полагается на значения функции, которые теперь обманывают его.

SciPy (BFGS) и CG начинают выдавать предупреждения Desired error not necessarily achieved due to precision loss, что говорит о трудностях с достижением высокой точности в условиях шума.
"""


def noisy_function_wrapper(original_func: FuncType, noise_sigma: float) -> FuncType:
    """
    Обертка для добавления шума (Гауссова) к функции. Применяется к функции Бута

    original_func -- исходная функция
    noise_sigma -- стандартное отклонение шума
    """

    def noisy_func(*args: Any, **kwargs: Any) -> float:
        # убираем noise_sigma, чтобы не передавать его в original_func
        func_kwargs = {k: v for k, v in kwargs.items() if k != "noise_sigma"}

        return original_func(*args, **func_kwargs) + np.random.normal(0, noise_sigma)

    setattr(noisy_func, "_original_func", original_func)
    # Сохранил оригинальную функцию как атрибут, потому что я
    # буду ее доставать для того, чтобы графики были чистые и они зашумленные
    return noisy_func


"""
1. Функция Бута

Вывод: На простых функциях почти все методы сходятся, но их эффективность кардинально различается.

Лучшие методы:

Методы с поиском шага (LineSearch GS и Armijo LS) — очень эффективны. 
Они находят минимум всего за 1 итерацию, хотя и тратят больше вычислений функции на этой итерации (39 для GS, 4 для Armijo). Это показывает, что "умный" выбор шага может радикально сократить общее число шагов.
SciPy (BFGS) и SciPy (CG) также очень быстры (3 итерации), что подтверждает их статус как мощных стандартных оптимизаторов.
Слабые методы:
Градиентный спуск с малым постоянным шагом (GD (Const LR=0.001)) оказался слишком медленным и уперся в лимит в 2000 итераций, так и не достигнув нужной точности. Это  демонстрирует критическую важность правильного подбора скорости обучения.
"""


def booth_function(x: float, y: float) -> float:
    """
    Функция Бута.
    простая, гладкая, унимодальная функция
    f(x,y) = (x + 2y - 7)^2 + (2x + y - 5)^2
    Минимум f(1, 3) = 0.
    """
    term1 = (x + 2 * y - 7) ** 2
    term2 = (2 * x + y - 5) ** 2
    return term1 + term2


def booth_function_gradient(x: float, y: float):
    """Градиент функции Бута."""
    # df/dx = 2(x + 2y - 7)*1 + 2(2x + y - 5)*2 = 2x + 4y - 14 + 8x + 4y - 20 = 10x + 8y - 34
    # df/dy = 2(x + 2y - 7)*2 + 2(2x + y - 5)*1 = 4x + 8y - 28 + 4x + 2y - 10 = 8x + 10y - 38
    lib = _get_lib(x)
    grad_x = 10 * x + 8 * y - 34
    grad_y = 8 * x + 10 * y - 38
    return lib.array([grad_x, grad_y]) if lib == np else lib.stack([grad_x, grad_y])


"""
Вытянутая квадратичная долина

Вывод: 
Плохо обусловленные функции — это очень плохо для  простого градиентного спуска, но более продвинутые методы справляются с ними хорошо.

Наблюдения:
GD (Const LR=0.01)  расходится: значения улетают в 1.47e+12. Это  пример того, почему простой GD не всегда применим.

Методы с адаптивным шагом (ExpDecay, StepDecay) справляются и сходятся там, где постоянный шаг потерпел неудачу. Это показывает их повышенную робастность.

GD (LineSearch GS) и SciPy (BFGS, CG) показывают себя хорошо, находя минимум за очень малое число итераций (7-11). Они умеют адаптировать не только размер, но и направление шага, что позволяет им эффективно двигаться по дну "оврага".

"""


def elongated_valley_quadratic(
    x: float, y: float, x_c: float, y_c: float, a1: float, a2: float, a3: float
) -> float:
    """
    Вытянутая квадратичная долина - это сложная квадратичная функция,
    у нее плохая обусловленность (то есть сильно вытянутую форму с очень разными скоростями изменения в разных направлениях)
    Эта функция имитирует "овраг", что является классической проблемой для простого градиентного спуска.

    Для квадратичной функции обусловленность характеризуется числом обусловленности матрицы Гессе: κ = λmax / λmin
    где λmax и λmin - максимальное и минимальное собственные значения матрицы вторых производных.

    Минимум f(x_c, y_c) = 0.
    Параметры x_c, y_c: центр долины.
    Параметры a1, a2: "крутизна" по осям X и Y (относительно x_c, y_c).
    Параметр a3: степень связи/поворота.
    """
    term1 = a1 * (x - x_c) ** 2
    term2 = a2 * (y - y_c) ** 2
    term3 = a3 * (x - x_c) * (y - y_c)
    return term1 + term2 + term3


def elongated_valley_quadratic_gradient(
    x: float, y: float, x_c: float, y_c: float, a1: float, a2: float, a3: float
):
    """Градиент вытянутой квадратичной функции."""
    lib = _get_lib(x)
    grad_x = 2 * a1 * (x - x_c) + a3 * (y - y_c)
    grad_y = 2 * a2 * (y - y_c) + a3 * (x - x_c)
    return lib.array([grad_x, grad_y]) if lib == np else lib.stack([grad_x, grad_y])


"""
Итоговые выводы

Нет "серебряной пули": Выбор оптимизатора сильно зависит от задачи. SciPy (BFGS) показал себя как самый универсальный и мощный метод для гладких функций, но и он пасует перед сильным шумом или сложной геометрией (предупреждения о потере точности на Экли).

Адаптация — ключ к успеху: Методы, которые адаптируют шаг (LineSearch, Armijo, ExpDecay), на порядок робастнее простого GD с постоянным шагом. Они могут справиться с плохой обусловленностью и даже помочь в поиске глобального минимума.

Цена адаптации: За робастность приходится платить. Методы с линейным поиском (LineSearch GS, Armijo) требуют значительно больше вычислений функции на каждой итерации, что может быть критично, если ее вычисление является дорогостоящим.

Ваши реализации конкурентоспособны: Ваши методы с линейным поиском (GS и Armijo) показали выдающиеся результаты, зачастую не уступая, а в некоторых аспектах (поиск глобального минимума Экли) даже превосходя стандартные имплементации. Реализованные вами предохранители корректно отработали в сложных ситуациях.
"""
