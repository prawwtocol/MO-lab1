"""
Существует множество стратегий выбора шага (learning rate scheduling) для метода градиентного спуска. Выбор подходящей стратегии может значительно повлиять на скорость сходимости алгоритма и качество найденного решения.

Зачем выбирать шаг

Слишком большой шаг: Мы можем "перепрыгнуть" минимум и начать "скакать" по склонам долины, так и не попав в нее.
Слишком маленький шаг: Спуск будет невероятно медленным, и мы можем не дойти до минимума за отведенное время.
"""

import numpy as np

from custom_types import LearningRateScheduler


def constant_lr(initial_lr: float) -> LearningRateScheduler:
    """Постоянный шаг
    Формула: α_k = α_0 (где α_k – шаг на k-й итерации, α_0 – начальный шаг).

    Преимущества: Простота реализации.
    Недостатки: Сложно подобрать оптимальное значение. Слишком большой шаг может привести к расходимости или "перепрыгиванию" через минимум. Слишком маленький шаг замедляет сходимость.
    """
    return lambda iteration: initial_lr


def exponential_decay_lr(
    initial_lr: float, decay_rate: float, decay_steps: int
) -> LearningRateScheduler:
    """Экспоненциальное затухание шага:
    lr = initial_lr * decay_rate^(iteration / decay_steps)

    Преимущества: Позволяет более точно настроиться на минимум.
    Недостатки: Требует подбора дополнительных гиперпараметров (начальный шаг, скорость затухания).

    """

    return lambda iteration: initial_lr * (decay_rate ** (iteration / decay_steps))


def step_decay_lr(
    initial_lr: float, drop_rate: float, epochs_drop: int
) -> LearningRateScheduler:
    """Ступенчатое уменьшение шага

    Шаг остается постоянным в течение определенного числа эпох, а затем уменьшается на некоторый коэффициент.
    Преимущества: Дает алгоритму время "устояться" с текущим шагом перед его уменьшением.
    Недостатки: Требует подбора начального шага, коэффициента уменьшения и частоты уменьшения.
    """

    return lambda iteration: initial_lr * drop_rate ** np.floor(
        (1 + iteration) / epochs_drop
    )
